{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('huri_whakatau': conda)"
  },
  "interpreter": {
   "hash": "e601707af480cebf7e51d900887f935fefdedfb11ef14622244ade15df2d9e9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['00-readme.txt',\n",
       " 'correlation.pl',\n",
       " 'STS.gs.MSRpar.txt',\n",
       " 'STS.gs.MSRvid.txt',\n",
       " 'STS.gs.SMTeuroparl.txt',\n",
       " 'STS.input.MSRpar.txt',\n",
       " 'STS.input.MSRvid.txt',\n",
       " 'STS.input.SMTeuroparl.txt',\n",
       " 'STS.output.MSRpar.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_files(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        raise RuntimeError(f\"{directory} does not exist.\")\n",
    "\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(directory):\n",
    "        files.extend(filenames)\n",
    "    return files\n",
    "\n",
    "get_files(\"../data/inference/stsb1/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 sent1  \\\n",
       "0    But other sources close to the sale said Viven...   \n",
       "1    Micron has declared its first quarterly profit...   \n",
       "2    The fines are part of failed Republican effort...   \n",
       "3    The American Anglican Council, which represent...   \n",
       "4    The tech-loaded Nasdaq composite rose 20.96 po...   \n",
       "..                                                 ...   \n",
       "729  Action is needed quickly, which is why we deci...   \n",
       "730  One could indeed wish for more and for improve...   \n",
       "731           (Parliament accepted the oral amendment)   \n",
       "732  - My party has serious reservations about Comm...   \n",
       "733                                 He saw a red rose.   \n",
       "\n",
       "                                                 sent2  score  data_suffix  \n",
       "0    But other sources close to the sale said Viven...   4.00       MSRpar  \n",
       "1    Micron's numbers also marked the first quarter...   3.75       MSRpar  \n",
       "2    Perry said he backs the Senate's efforts, incl...   2.80       MSRpar  \n",
       "3    The American Anglican Council, which represent...   3.40       MSRpar  \n",
       "4    The technology-laced Nasdaq Composite Index <....   2.40       MSRpar  \n",
       "..                                                 ...    ...          ...  \n",
       "729  It is urgent and that is why we have decided t...   5.00  SMTeuroparl  \n",
       "730  We can actually want more and better, but I th...   4.80  SMTeuroparl  \n",
       "731           (Parliament accepted the oral amendment)   5.00  SMTeuroparl  \n",
       "732  My party serious reservations about the regula...   4.80  SMTeuroparl  \n",
       "733                       He drove a rose colored car.   0.80  SMTeuroparl  \n",
       "\n",
       "[2234 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent1</th>\n      <th>sent2</th>\n      <th>score</th>\n      <th>data_suffix</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>But other sources close to the sale said Viven...</td>\n      <td>But other sources close to the sale said Viven...</td>\n      <td>4.00</td>\n      <td>MSRpar</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Micron has declared its first quarterly profit...</td>\n      <td>Micron's numbers also marked the first quarter...</td>\n      <td>3.75</td>\n      <td>MSRpar</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The fines are part of failed Republican effort...</td>\n      <td>Perry said he backs the Senate's efforts, incl...</td>\n      <td>2.80</td>\n      <td>MSRpar</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The American Anglican Council, which represent...</td>\n      <td>The American Anglican Council, which represent...</td>\n      <td>3.40</td>\n      <td>MSRpar</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n      <td>2.40</td>\n      <td>MSRpar</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>729</th>\n      <td>Action is needed quickly, which is why we deci...</td>\n      <td>It is urgent and that is why we have decided t...</td>\n      <td>5.00</td>\n      <td>SMTeuroparl</td>\n    </tr>\n    <tr>\n      <th>730</th>\n      <td>One could indeed wish for more and for improve...</td>\n      <td>We can actually want more and better, but I th...</td>\n      <td>4.80</td>\n      <td>SMTeuroparl</td>\n    </tr>\n    <tr>\n      <th>731</th>\n      <td>(Parliament accepted the oral amendment)</td>\n      <td>(Parliament accepted the oral amendment)</td>\n      <td>5.00</td>\n      <td>SMTeuroparl</td>\n    </tr>\n    <tr>\n      <th>732</th>\n      <td>- My party has serious reservations about Comm...</td>\n      <td>My party serious reservations about the regula...</td>\n      <td>4.80</td>\n      <td>SMTeuroparl</td>\n    </tr>\n    <tr>\n      <th>733</th>\n      <td>He saw a red rose.</td>\n      <td>He drove a rose colored car.</td>\n      <td>0.80</td>\n      <td>SMTeuroparl</td>\n    </tr>\n  </tbody>\n</table>\n<p>2234 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def load_sts_subset(input_fname, gs_fname, directory):\n",
    "    # extracts file id with re.findall\n",
    "    sts_suffix_re = re.compile(r\"STS\\.(?:gs|input)\\.((?:\\w{1,}\\.)?\\w{1,})\\.txt\")\n",
    "    input_suffix = sts_suffix_re.findall(input_fname)[0]\n",
    "    gs_suffix = sts_suffix_re.findall(gs_fname)[0]\n",
    "\n",
    "    assert input_suffix == gs_suffix, \"Suffixes don't match, logic wrong\"\n",
    "\n",
    "    sentences = pd.read_csv(\n",
    "        os.path.join(directory, input_fname),\n",
    "        sep = \"\\\\t\",\n",
    "        names = [\"sent1\", \"sent2\"],\n",
    "        engine = \"python\")\n",
    "\n",
    "    gold_standard = pd.read_csv(\n",
    "        os.path.join(directory, gs_fname),\n",
    "        names = [\"score\"])\n",
    "\n",
    "    concat_frame = pd.concat([sentences, gold_standard], axis=1)\n",
    "    concat_frame[\"data_suffix\"] = input_suffix\n",
    "\n",
    "    return concat_frame\n",
    "\n",
    "\n",
    "def load_sts_from_dir(data_dir):\n",
    "    dir_files = get_files(data_dir)\n",
    "    sts_file_regex = re.compile(r\"STS\\.input\\.(\\w{1,}\\.)?(\\w{1,})\\.txt\")\n",
    "\n",
    "    # files that match the sts input data format\n",
    "    valid_sts_files = [\n",
    "        fname for fname in dir_files\n",
    "        if bool(sts_file_regex.search(fname))\n",
    "    ]\n",
    "\n",
    "    dir_files = set(dir_files)\n",
    "    datasets = []\n",
    "    for sts_fname in valid_sts_files:\n",
    "        gs_fname = re.sub(r\"(?<=STS\\.)input\", \"gs\", sts_fname)\n",
    "\n",
    "        # skip files that don't have gold standard\n",
    "        if gs_fname not in dir_files:\n",
    "            print(f\"skipping: {sts_fname}, cant find GS equivalent\")\n",
    "            continue\n",
    "\n",
    "        datasets.append(load_sts_subset(sts_fname, gs_fname, data_dir))\n",
    "\n",
    "    return pd.concat(datasets)\n",
    "\n",
    "\n",
    "def load_stsb_data(base_dir=\"../data/inference/stsb1\"):\n",
    "    tr_sentences = load_sts_from_dir(f\"{base_dir}/train\")\n",
    "    tst_sentences = load_sts_from_dir(f\"{base_dir}/test-gold\")\n",
    "\n",
    "    return tr_sentences, tst_sentences\n",
    "\n",
    "train_sentences, test_sentences = load_stsb_data()\n",
    "\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x2d98296d490>"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def tokenize_documents(sentences):\n",
    "    return [sentence.strip().lower().split() for sentence in sentences]\n",
    "\n",
    "\n",
    "def train_doc2vec(sentences, seed=0XDECAF, **doc2vec_kwargs):\n",
    "    stoplist = set(stopwords.words(\"english\"))\n",
    "    tokenized = tokenize_documents(sentences)\n",
    "    train_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized)]\n",
    "\n",
    "    return (\n",
    "        Doc2Vec(\n",
    "            train_docs,\n",
    "            seed=seed,\n",
    "            **doc2vec_kwargs\n",
    "        ))\n",
    "\n",
    "train_documents = np.concatenate((train_sentences.sent1.values, train_sentences.sent2.values))\n",
    "doc2vec_model = train_doc2vec(\n",
    "    train_documents,\n",
    "    vector_size=100,\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    epochs=200)\n",
    "\n",
    "doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.27336246  0.01929293 -0.43676952  0.6697727  -0.677531    1.1908083\n  0.31147757 -0.6999784  -0.43487403 -0.38292193  0.5500924  -0.26284248\n  0.40142235  0.01385904 -0.17844252 -1.4803333   0.5652604  -0.41115785\n -1.1323328  -0.17272663  0.22103062 -0.5254174   0.02997912 -0.6717148\n  0.12126261 -0.5301277   0.31409562  0.26459464 -0.6107148  -0.11397837\n -0.02911393 -0.47717014  0.02739717  0.0538495   1.2027074  -0.14654395\n -0.19840205 -0.5300882  -0.12192903  0.32900253  0.64409846 -0.16664167\n -0.8011557  -0.355054   -0.4767804   0.7084554  -1.1959565   0.7588401\n -0.31184205 -0.5281979  -0.44490167  0.04867037  0.75815904 -0.43769515\n -0.31825233 -0.49941498 -0.10314978 -0.49450213  0.33938453  0.03741687\n  0.19376202  0.13575885 -0.9750473  -1.0288007  -0.6467795  -0.702557\n  0.1861674   0.4833676  -0.7172501   0.14684117  0.24359186  0.24011372\n -0.5823613   0.7358891   0.4841842  -0.58848137 -1.1988727  -0.15141842\n -0.43829936  0.25514224 -0.39504713 -1.0776379   0.4051799  -0.17964521\n -0.09988399  1.2739587   1.4073838   0.5546676  -1.4436156  -0.30475694\n  0.08920267 -0.28091583  0.4364135   0.12329771  0.5569769   0.22920667\n  1.1688616   0.25043288 -0.6525972   1.4757627 ]\n[-1.06448783e-02  2.77317725e-02 -2.18442231e-01  3.11417013e-01\n -3.59422565e-01  1.03101707e+00  3.85959297e-01  7.23845065e-02\n -2.18354791e-01 -1.68939214e-02  5.41455984e-01 -6.05724514e-01\n -2.89720416e-01  2.20039845e-01  1.07208230e-01 -8.19450140e-01\n  1.03829041e-01 -5.12163401e-01 -4.56729770e-01  1.63620144e-01\n  4.83686626e-01 -3.84158462e-01 -3.11419275e-02  1.78725958e-01\n -3.68504710e-02  2.72464484e-01  1.48411784e-02 -8.85774344e-02\n  1.62742227e-01  1.11196004e-01  3.68052840e-01 -1.09089351e+00\n  1.88854963e-01 -3.57349025e-04  7.95747399e-01 -2.60255933e-01\n -1.78708926e-01 -4.56562415e-02  2.79908836e-01  6.64965138e-02\n  3.07461739e-01  3.30941021e-01 -5.32867134e-01 -6.09939277e-01\n -4.92425263e-01  5.61624825e-01 -1.12628758e+00  6.92445552e-03\n  3.24652612e-01 -5.03481925e-01 -6.54972970e-01  5.72963595e-01\n  3.53793114e-01 -6.48029268e-01 -2.03323007e-01  9.45486724e-02\n  2.82818973e-01 -2.99317151e-01 -9.90324438e-01 -3.87105018e-01\n -9.95363370e-02  6.59777701e-01 -6.90825522e-01 -2.00427562e-01\n -1.09542274e+00 -1.02901153e-01 -7.41210803e-02  4.69794929e-01\n -4.99712341e-02 -2.69135892e-01  2.37315401e-01 -3.30629170e-01\n -1.91417828e-01  8.85336936e-01  3.40562671e-01 -2.33237401e-01\n -4.16048199e-01 -1.72534928e-01 -4.51921761e-01  6.67819604e-02\n -6.30660474e-01 -1.48422897e-01  2.47691199e-01 -1.44371006e-03\n  2.42445767e-01  6.53217316e-01  1.31952024e+00  2.92229235e-01\n -6.08101666e-01 -4.90598649e-01 -4.90547091e-01 -2.90343344e-01\n  1.04738940e-02  1.03021696e-01  2.61602223e-01 -4.48166341e-01\n  1.04001987e+00  2.08099708e-01 -5.70880234e-01  1.19917715e+00]\n0.71613973\n"
     ]
    }
   ],
   "source": [
    "cos_sim = lambda a, b: dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def infer_doc2vec(d2v_model, sentences):\n",
    "    tokenized_documents = tokenize_documents(sentences)\n",
    "    return [\n",
    "        d2v_model.infer_vector(document_words)\n",
    "        for document_words in tokenized_documents\n",
    "    ]\n",
    "\n",
    "sent1_embeddings = infer_doc2vec(doc2vec_model, train_sentences.sent1.values)\n",
    "sent2_embeddings = infer_doc2vec(doc2vec_model, train_sentences.sent2.values)\n",
    "\n",
    "print(sent1_embeddings[0])\n",
    "print(sent2_embeddings[0])\n",
    "print(cos_sim(sent1_embeddings[0], sent2_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4.2903495, 3.9164648, 4.30869  , 4.4133883, 3.5119689, 4.0685463,\n",
       "       4.149051 , 4.253704 , 4.111351 , 4.250328 , 4.3949933, 4.8942895,\n",
       "       4.591283 , 4.8508935, 4.5573044, 4.6417017, 4.3985767, 4.24268  ,\n",
       "       3.9623451, 4.7027354], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "from numpy import dot, ndarray\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def compare_embeddings(embedding_one, embedding_two):\n",
    "    emb_one_is_list = isinstance(embedding_one[0], (list, ndarray))\n",
    "    emb_two_is_list = isinstance(embedding_two[0], (list, ndarray))\n",
    "\n",
    "    # pairwise compare all embeddings\n",
    "    if emb_one_is_list and emb_two_is_list:\n",
    "        if len(embedding_one) != len(embedding_two):\n",
    "            raise RuntimeError(\n",
    "                \"embedding one and embedding two lists not of equal length\"\n",
    "            )\n",
    "        return np.array([\n",
    "            cos_sim(e1, e2)\n",
    "            for e1, e2 in zip(embedding_one, embedding_two)\n",
    "        ])\n",
    "    # embedding one compared against all of embedding two\n",
    "    elif emb_two_is_list:\n",
    "        return [cos_sim(embedding_one, e2) for e2 in embedding_two]\n",
    "    # compare embedding one against embedding two\n",
    "    elif not emb_one_is_list:\n",
    "        return cos_sim(embedding_one, embedding_two)\n",
    "\n",
    "    # embedding one is a list of embeddings but embedding two is one embedding\n",
    "    raise RuntimeError(\n",
    "        \"Either Embedding one and two are lists of embeddings, else only embedding two.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def rescale_numeric(cos_dist, cur_min=-1, cur_max=1, new_min=0, new_max=5):\n",
    "    # percent of measurement on current scale\n",
    "    cur_perc = (cos_dist - cur_min) / (cur_max - cur_min)\n",
    "\n",
    "    # for scaling the measurement to the new range\n",
    "    scaling_fct = (new_max - new_min) + new_min\n",
    "    return cur_perc * scaling_fct\n",
    "\n",
    "\n",
    "def get_embedding_distances(s1_embeddings, s2_embeddings, scale=True):\n",
    "    distances = compare_embeddings(s1_embeddings, s2_embeddings)\n",
    "\n",
    "    # return cosine similarity unscaled\n",
    "    if not scale:\n",
    "        return distances\n",
    "\n",
    "    return rescale_numeric(distances)\n",
    "\n",
    "\n",
    "embedding_scores = get_embedding_distances(sent1_embeddings, sent2_embeddings, scale=True)\n",
    "embedding_scores[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2234"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "len(embedding_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3679310312104844"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# todo: proper benchmark\n",
    "def eval_embedding_correlation(t, truth_vector):\n",
    "    return pearsonr(t, truth_vector)[0]\n",
    "\n",
    "eval_embedding_correlation(embedding_scores, train_sentences.score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: end-to-end pipeline for benchmarking models\n",
    "#  def embedding_pipeline(tr, test, encode_func, preprocess_func):"
   ]
  }
 ]
}